{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11454239,"sourceType":"datasetVersion","datasetId":7176873}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Importing Required Libraries","metadata":{}},{"cell_type":"code","source":"from tqdm.auto import tqdm\nimport random\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import models, datasets, transforms\nfrom torch.utils.data import DataLoader, Subset, ConcatDataset\nimport pathlib\nimport wandb\nimport gc","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T18:41:45.282139Z","iopub.execute_input":"2025-04-21T18:41:45.282413Z","iopub.status.idle":"2025-04-21T18:41:54.667551Z","shell.execute_reply.started":"2025-04-21T18:41:45.282382Z","shell.execute_reply":"2025-04-21T18:41:54.667016Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# Configure matrix multiplication precision for better performance\nif hasattr(torch, 'set_float32_matmul_precision'):\n    torch.set_float32_matmul_precision('medium')\n\n# Initialize Weights & Biases for experiment tracking\nwandb.login(key=\"7f46816d45e3df192c3053bab59032e9d710fef4\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T18:41:54.668970Z","iopub.execute_input":"2025-04-21T18:41:54.669243Z","iopub.status.idle":"2025-04-21T18:42:00.562669Z","shell.execute_reply.started":"2025-04-21T18:41:54.669216Z","shell.execute_reply":"2025-04-21T18:42:00.562100Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcs24m034\u001b[0m (\u001b[33mcs24m034-indian-institute-of-technology-madras\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":2},{"cell_type":"markdown","source":"# Image Transformation Pipeline\nThe following function defines a preprocessing pipeline for image data using torchvision.transforms. It supports optional data augmentation for training.\n\n","metadata":{}},{"cell_type":"code","source":"def get_transforms(mean, std, augment=False):\n    \"\"\"Create image transformations pipeline\"\"\"\n    base_transforms = [\n        transforms.Resize((224, 224)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=torch.tensor(mean), std=torch.tensor(std))\n    ]\n    \n    if augment:\n        return transforms.Compose([\n            transforms.Resize((224, 224)),\n            transforms.RandomHorizontalFlip(),\n            transforms.RandomRotation(30),\n            *base_transforms[1:]\n        ])\n    \n    return transforms.Compose(base_transforms)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T18:42:00.565528Z","iopub.execute_input":"2025-04-21T18:42:00.565719Z","iopub.status.idle":"2025-04-21T18:42:00.570536Z","shell.execute_reply.started":"2025-04-21T18:42:00.565703Z","shell.execute_reply":"2025-04-21T18:42:00.569768Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"# Dataset Split with Class Balance\nThis function is designed to split a dataset into training and validation subsets while maintaining class balance. It ensures each class is equally represented in both subsets, which is crucial for unbiased evaluation.","metadata":{}},{"cell_type":"code","source":"def split_dataset(dataset, num_classes, validation_ratio=0.2):\n    \"\"\"Divide dataset into training and validation sets with balanced classes\"\"\"\n    class_indices = {cls: [] for cls in range(num_classes)}\n    \n    for idx, label in enumerate(dataset.targets):\n        class_indices[label].append(idx)\n    \n    validation_indices = []\n    for indices in class_indices.values():\n        val_count = int(len(indices) * validation_ratio)\n        validation_indices += random.sample(indices, val_count)\n    \n    train_indices = list(set(range(len(dataset))) - set(validation_indices))\n    return Subset(dataset, train_indices), Subset(dataset, validation_indices)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T18:42:00.572078Z","iopub.execute_input":"2025-04-21T18:42:00.572348Z","iopub.status.idle":"2025-04-21T18:42:00.587551Z","shell.execute_reply.started":"2025-04-21T18:42:00.572330Z","shell.execute_reply":"2025-04-21T18:42:00.586943Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"# Preparing Data Loaders with Optional Augmentation\nThis function sets up PyTorch data loaders for training, validation, and testing from an image dataset structured using ImageFolder. It applies normalization, supports optional data augmentation, splits the training set with class balance, and extracts class labels from the directory structure.","metadata":{}},{"cell_type":"code","source":"def prepare_data_loaders(dataset_path, num_classes=10, augment_data=False, batch_size=32):\n    \"\"\"Prepare data loaders for training, validation, and testing\"\"\"\n    # Precomputed normalization parameters\n    channel_means = [0.4708, 0.4596, 0.3891]\n    channel_stds = [0.1951, 0.1892, 0.1859]\n\n    # Create transformation pipelines\n    train_transform = get_transforms(channel_means, channel_stds)\n    test_transform = get_transforms(channel_means, channel_stds)\n    augment_transform = get_transforms(channel_means, channel_stds, augment=True)\n\n    # Load datasets\n    train_dataset = datasets.ImageFolder(\n        root=f\"{dataset_path}train\", \n        transform=train_transform\n    )\n    test_dataset = datasets.ImageFolder(\n        root=f\"{dataset_path}val\", \n        transform=test_transform\n    )\n\n    # Split into training and validation\n    train_subset, val_subset = split_dataset(train_dataset, num_classes)\n\n    # Configure data loader parameters\n    loader_config = {\n        \"batch_size\": batch_size,\n        \"num_workers\": 4,\n        \"pin_memory\": True,\n        \"persistent_workers\": True\n    }\n\n    # Create data loaders\n    train_loader = DataLoader(train_subset, shuffle=True, **loader_config)\n    val_loader = DataLoader(val_subset, shuffle=False, **loader_config)\n    test_loader = DataLoader(test_dataset, shuffle=False, **loader_config)\n\n    # Add augmented data if enabled\n    if augment_data:\n        augmented_data = datasets.ImageFolder(\n            root=f\"{dataset_path}train\", \n            transform=augment_transform\n        )\n        combined_data = ConcatDataset([train_subset, augmented_data])\n        train_loader = DataLoader(combined_data, shuffle=True, **loader_config)\n\n    # Extract class names from directory structure\n    class_names = sorted(\n        entry.name for entry in pathlib.Path(f\"{dataset_path}train\").iterdir() \n        if entry.is_dir() and entry.name != \".DS_Store\"\n    )\n\n    return train_loader, val_loader, test_loader, class_names","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T18:42:00.588264Z","iopub.execute_input":"2025-04-21T18:42:00.588478Z","iopub.status.idle":"2025-04-21T18:42:00.605114Z","shell.execute_reply.started":"2025-04-21T18:42:00.588461Z","shell.execute_reply":"2025-04-21T18:42:00.604513Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"# Training Loop with Mixed Precision and Validation\nThis function trains a neural network model using a given dataset and optimizer for a specified number of epochs. It includes mixed precision training for efficiency, tracks performance metrics, logs results to Weights & Biases, performs validation after each epoch, and evaluates final performance on the test set.","metadata":{}},{"cell_type":"code","source":"def train_model(device, train_loader, val_loader, test_loader, model, epochs=10):\n    \"\"\"Train the neural network model\"\"\"\n    loss_function = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\n    \n    # Enable mixed precision training if available\n    precision_scaler = torch.cuda.amp.GradScaler(enabled=torch.cuda.is_available())\n\n    for epoch in tqdm(range(epochs)):\n        model.train()\n        epoch_loss = 0.0\n        correct_predictions = 0\n        total_samples = 0\n        \n        for inputs, labels in tqdm(train_loader):\n            inputs = inputs.to(device, non_blocking=True)\n            labels = labels.to(device, non_blocking=True)\n            \n            optimizer.zero_grad(set_to_none=True)\n            \n            # Mixed precision forward pass\n            with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n                outputs = model(inputs)\n                loss = loss_function(outputs, labels)\n            \n            # Backpropagation with precision scaling\n            precision_scaler.scale(loss).backward()\n            precision_scaler.step(optimizer)\n            precision_scaler.update()\n\n            # Track performance metrics\n            with torch.no_grad():\n                _, predicted = torch.max(outputs, 1)\n                correct_predictions += (predicted == labels).sum().item()\n                total_samples += labels.size(0)\n                epoch_loss += loss.item() * inputs.size(0)\n        \n        # Calculate epoch statistics\n        avg_loss = epoch_loss / len(train_loader.dataset)\n        accuracy = correct_predictions / total_samples\n        print(f\"Epoch [{epoch+1}/{epochs}], Accuracy: {accuracy * 100:.2f}%, Loss: {avg_loss:.4f}\")\n        wandb.log({'accuracy': accuracy, 'loss': avg_loss})\n\n        # Validation phase\n        model.eval()\n        with torch.no_grad():\n            val_correct = 0\n            val_total = 0\n            val_loss = 0.0\n            \n            for val_inputs, val_labels in tqdm(val_loader):\n                val_inputs = val_inputs.to(device, non_blocking=True)\n                val_labels = val_labels.to(device, non_blocking=True)\n                \n                with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n                    val_outputs = model(val_inputs)\n                    current_loss = loss_function(val_outputs, val_labels)\n\n                _, val_predicted = torch.max(val_outputs, 1)\n                val_correct += (val_predicted == val_labels).sum().item()\n                val_total += val_labels.size(0)\n                val_loss += current_loss.item() * val_inputs.size(0)\n\n            val_loss /= len(val_loader.dataset)\n            val_accuracy = val_correct / val_total\n            print(f\"Validation Accuracy: {val_accuracy * 100:.2f}%, Loss: {val_loss:.4f}\")\n            wandb.log({'val_accuracy': val_accuracy, 'val_loss': val_loss})\n\n        # Clean up memory\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n            gc.collect()\n\n        # Final evaluation on test set\n        if epoch == epochs-1:\n            model.eval()\n            with torch.no_grad():\n                test_correct = 0\n                test_total = 0\n                test_loss = 0.0\n                \n                for test_inputs, test_labels in tqdm(test_loader):\n                    test_inputs = test_inputs.to(device, non_blocking=True)\n                    test_labels = test_labels.to(device, non_blocking=True)\n                    \n                    with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n                        test_outputs = model(test_inputs)\n                        current_test_loss = loss_function(test_outputs, test_labels)\n    \n                    _, test_predicted = torch.max(test_outputs, 1)\n                    test_correct += (test_predicted == test_labels).sum().item()\n                    test_total += test_labels.size(0)\n                    test_loss += current_test_loss.item() * test_inputs.size(0)\n    \n                test_loss /= len(test_loader.dataset)\n                test_accuracy = test_correct / test_total\n                print(f\"Test Accuracy: {test_accuracy * 100:.2f}%, Loss: {test_loss:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T18:42:00.605942Z","iopub.execute_input":"2025-04-21T18:42:00.606217Z","iopub.status.idle":"2025-04-21T18:42:00.628497Z","shell.execute_reply.started":"2025-04-21T18:42:00.606193Z","shell.execute_reply":"2025-04-21T18:42:00.627806Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"# Parameter Freezing Utilities for Transfer Learning\nThis code provides helper functions to control which parts of a neural network are trainable. You can freeze all parameters, freeze only a specified number of layers, or re-enable all parameters for fine-tuning during transfer learning.","metadata":{}},{"cell_type":"code","source":"def freeze_parameters(model):\n    \"\"\"Disable gradient computation for all parameters\"\"\"\n    for param in model.parameters():\n        param.requires_grad = False\n\ndef freeze_partial_model(model, freeze_count):\n    \"\"\"Freeze specified number of layers\"\"\"\n    frozen = 0\n    for param in model.parameters():\n        if frozen < freeze_count:\n            param.requires_grad = False\n            frozen += 1\n        else:\n            break\n\ndef enable_all_parameters(model):\n    \"\"\"Enable gradient computation for all parameters\"\"\"\n    for param in model.parameters():\n        param.requires_grad = True","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T18:42:00.629201Z","iopub.execute_input":"2025-04-21T18:42:00.629469Z","iopub.status.idle":"2025-04-21T18:42:00.646960Z","shell.execute_reply.started":"2025-04-21T18:42:00.629442Z","shell.execute_reply":"2025-04-21T18:42:00.646314Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"# End-to-End Training Pipeline with Fine-Tuning Options\nThis function runs the complete training workflow, including data loading, model initialization, optional layer freezing, and logging with Weights & Biases. It supports different fine-tuning strategies using GoogLeNet and executes the pipeline on GPU if available.","metadata":{}},{"cell_type":"code","source":"def execute_pipeline():\n    \"\"\"Main execution function\"\"\"\n    data_path = '/kaggle/input/nature/inaturalist_12K/'  \n    augmentation_enabled = True\n    batch_size = 64\n    num_classes = 10\n    fine_tune_option = 2\n    layers_to_freeze = 12\n\n    def run_training():\n        with wandb.init(project=\"Testing_3\") as experiment:\n            config = wandb.config\n            experiment_name = f\"aug_{augmentation_enabled}_bs_{batch_size}_fine_tune_{fine_tune_option}\"\n            if fine_tune_option == 1:\n                experiment_name += \"_freeze_all\"\n            elif fine_tune_option == 3:\n                experiment_name += \"_freeze_none\"\n            else:\n                experiment_name += f\"_freeze_{layers_to_freeze}\"\n\n            wandb.run.name = experiment_name\n            \n            train_loader, val_loader, test_loader, class_names = prepare_data_loaders(\n                data_path, \n                num_classes=num_classes, \n                augment_data=augmentation_enabled, \n                batch_size=batch_size\n            )\n            \n            print(f\"Training batches: {len(train_loader)}\")\n            print(f\"Validation batches: {len(val_loader)}\")\n            print(f\"Test batches: {len(test_loader)}\")\n    \n            # Set up computation device\n            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n            print(f\"Using device: {device}\")\n        \n            # Initialize model with pretrained weights\n            try:\n                model = models.googlenet(weights='IMAGENET1K_V1')\n            except TypeError:\n                # Fallback for older PyTorch versions\n                model = models.googlenet(pretrained=True)\n            model = model.to(device)\n            \n            # Enable model compilation if available\n            if hasattr(torch, 'compile') and torch.cuda.is_available():\n                print(\"Using model compilation for improved performance\")\n\n            # Apply selected fine-tuning strategy\n            if fine_tune_option == 1:\n                freeze_parameters(model)\n                model.fc = nn.Linear(model.fc.in_features, num_classes)\n                model = model.to(device)\n                train_model(device, train_loader, val_loader, test_loader, model, epochs=5)\n            \n            elif fine_tune_option == 2:\n                freeze_partial_model(model, layers_to_freeze)\n                model.fc = nn.Linear(model.fc.in_features, num_classes)\n                model = model.to(device)\n                train_model(device, train_loader, val_loader, test_loader, model, epochs=5)\n\n            else:\n                enable_all_parameters(model)\n                model.fc = nn.Linear(model.fc.in_features, num_classes)\n                model = model.to(device)\n                train_model(device, train_loader, val_loader, test_loader, model, epochs=5)\n                \n            # Clean up GPU resources\n            if torch.cuda.is_available():\n                torch.cuda.empty_cache()\n                \n    run_training()\n    wandb.finish()\n    \nif __name__ == \"__main__\":\n    execute_pipeline()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T18:42:00.647693Z","iopub.execute_input":"2025-04-21T18:42:00.647994Z","iopub.status.idle":"2025-04-21T18:51:23.173189Z","shell.execute_reply.started":"2025-04-21T18:42:00.647973Z","shell.execute_reply":"2025-04-21T18:51:23.172258Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250421_184200-j6ynidi9</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/cs24m034-indian-institute-of-technology-madras/Testing_3/runs/j6ynidi9' target=\"_blank\">floral-dust-8</a></strong> to <a href='https://wandb.ai/cs24m034-indian-institute-of-technology-madras/Testing_3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/cs24m034-indian-institute-of-technology-madras/Testing_3' target=\"_blank\">https://wandb.ai/cs24m034-indian-institute-of-technology-madras/Testing_3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/cs24m034-indian-institute-of-technology-madras/Testing_3/runs/j6ynidi9' target=\"_blank\">https://wandb.ai/cs24m034-indian-institute-of-technology-madras/Testing_3/runs/j6ynidi9</a>"},"metadata":{}},{"name":"stdout","text":"Training batches: 282\nValidation batches: 32\nTest batches: 32\nUsing device: cuda\n","output_type":"stream"},{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/googlenet-1378be20.pth\" to /root/.cache/torch/hub/checkpoints/googlenet-1378be20.pth\n100%|██████████| 49.7M/49.7M [00:00<00:00, 216MB/s]\n","output_type":"stream"},{"name":"stdout","text":"Using model compilation for improved performance\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_31/2434531658.py:7: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  precision_scaler = torch.cuda.amp.GradScaler(enabled=torch.cuda.is_available())\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aa835e65794141a8accca62999acd9ee"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/282 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2af3dbd07ffe48dab0e87edcd206852a"}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_31/2434531658.py:22: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/5], Accuracy: 62.19%, Loss: 1.1152\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/32 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"59b69e8efc284b9d9cef111b1dda1cd2"}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_31/2434531658.py:55: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n","output_type":"stream"},{"name":"stdout","text":"Validation Accuracy: 63.38%, Loss: 1.1325\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/282 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6501a9181aa04dab90567432a96f624c"}},"metadata":{}},{"name":"stdout","text":"Epoch [2/5], Accuracy: 73.44%, Loss: 0.7869\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/32 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2d3ec92f7d9f48c49320d5d371d50a42"}},"metadata":{}},{"name":"stdout","text":"Validation Accuracy: 70.64%, Loss: 0.8939\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/282 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"62b7e6d831d04e70b05870cb1ff81466"}},"metadata":{}},{"name":"stdout","text":"Epoch [3/5], Accuracy: 79.40%, Loss: 0.6206\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/32 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"353d043522cd470c85b64d2a8e19b75d"}},"metadata":{}},{"name":"stdout","text":"Validation Accuracy: 73.04%, Loss: 0.8443\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/282 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2c3ccb7873de4a5b8bb5dff99b1ae154"}},"metadata":{}},{"name":"stdout","text":"Epoch [4/5], Accuracy: 82.40%, Loss: 0.5174\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/32 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d231906d194048ec932281fda6fdc654"}},"metadata":{}},{"name":"stdout","text":"Validation Accuracy: 76.84%, Loss: 0.7382\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/282 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"41b575456ce94027aab6cfc71c6cc736"}},"metadata":{}},{"name":"stdout","text":"Epoch [5/5], Accuracy: 85.54%, Loss: 0.4324\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/32 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d9e00d5f04e8444db72d10d6fa0222b5"}},"metadata":{}},{"name":"stdout","text":"Validation Accuracy: 82.94%, Loss: 0.5066\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/32 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d00d189617d549f6be78251cfa3c5d74"}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_31/2434531658.py:86: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 69.50%, Loss: 1.0629\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>▁▄▆▇█</td></tr><tr><td>loss</td><td>█▅▃▂▁</td></tr><tr><td>val_accuracy</td><td>▁▄▄▆█</td></tr><tr><td>val_loss</td><td>█▅▅▄▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>0.85538</td></tr><tr><td>loss</td><td>0.43239</td></tr><tr><td>val_accuracy</td><td>0.82941</td></tr><tr><td>val_loss</td><td>0.50664</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">aug_True_bs_64_fine_tune_2_freeze_12</strong> at: <a href='https://wandb.ai/cs24m034-indian-institute-of-technology-madras/Testing_3/runs/j6ynidi9' target=\"_blank\">https://wandb.ai/cs24m034-indian-institute-of-technology-madras/Testing_3/runs/j6ynidi9</a><br> View project at: <a href='https://wandb.ai/cs24m034-indian-institute-of-technology-madras/Testing_3' target=\"_blank\">https://wandb.ai/cs24m034-indian-institute-of-technology-madras/Testing_3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250421_184200-j6ynidi9/logs</code>"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}